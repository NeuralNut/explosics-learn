{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mamba_ssm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mamba\n\u001b[1;32m      4\u001b[0m batch, length, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch, length, dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mamba_ssm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[-7.8456,  5.7759, -4.0456, -5.5398, -3.8413,  3.7264, -6.8066,  7.5594,\n",
      "         -0.9572,  7.0396]], grad_fn=<MmBackward0>)\n",
      "New State: tensor([[-0.7964,  1.0000, -0.7797,  0.9971, -0.9998, -1.0000,  0.9067, -0.9998,\n",
      "         -0.9986,  1.0000, -0.4037, -0.9912,  0.9935, -0.9994, -0.9562,  1.0000,\n",
      "         -0.4218,  1.0000, -1.0000,  1.0000]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, input_dim, state_dim):\n",
    "        super(MambaBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        # Define the matrices A, B, C as learnable parameters\n",
    "        self.A = nn.Parameter(torch.randn(state_dim, state_dim))\n",
    "        self.B = nn.Parameter(torch.randn(state_dim, input_dim))\n",
    "        self.C = nn.Parameter(torch.randn(input_dim, state_dim))\n",
    "\n",
    "        # Optional: Bias terms for more flexibility\n",
    "        self.bias = nn.Parameter(torch.randn(state_dim))\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        \"\"\"\n",
    "        Forward pass of the Mamba block.\n",
    "        x: Input tensor (batch_size, input_dim)\n",
    "        state: Previous state tensor (batch_size, state_dim)\n",
    "        \"\"\"\n",
    "        # Update state based on previous state and current input\n",
    "        # Ensure matrix multiplication dimensions align correctly\n",
    "        new_state = torch.tanh(torch.matmul(state, self.A.t()) + torch.matmul(x, self.B.t()) + self.bias)\n",
    "        \n",
    "        # Compute the output\n",
    "        output = torch.matmul(new_state, self.C.t())\n",
    "        return output, new_state\n",
    "\n",
    "# Example usage\n",
    "input_dim = 10\n",
    "state_dim = 20\n",
    "mamba_block = MambaBlock(input_dim, state_dim)\n",
    "\n",
    "# Random input and initial state\n",
    "x = torch.randn(1, input_dim)\n",
    "state = torch.randn(1, state_dim)\n",
    "\n",
    "output, new_state = mamba_block(x, state)\n",
    "print(\"Output:\", output)\n",
    "print(\"New State:\", new_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaBlock()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mamba_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mitch/miniconda3/envs/phd/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hey how are you doing?\\n\\nI'm so glad you're here.\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "input_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n",
    "\n",
    "out = model.generate(input_ids, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MambaForCausalLM                         --\n",
       "├─MambaModel: 1-1                        --\n",
       "│    └─Embedding: 2-1                    38,615,040\n",
       "│    └─ModuleList: 2-2                   --\n",
       "│    │    └─MambaBlock: 3-1              --\n",
       "│    │    │    └─MambaRMSNorm: 4-1       768\n",
       "│    │    │    └─MambaMixer: 4-2         26,112\n",
       "│    │    │    │    └─Conv1d: 5-1        7,680\n",
       "│    │    │    │    └─SiLU: 5-2          --\n",
       "│    │    │    │    └─Linear: 5-3        2,359,296\n",
       "│    │    │    │    └─Linear: 5-4        122,880\n",
       "│    │    │    │    └─Linear: 5-5        75,264\n",
       "│    │    │    │    └─Linear: 5-6        1,179,648\n",
       "│    │    └─MambaBlock: 3-2              --\n",
       "│    │    │    └─MambaRMSNorm: 4-3       768\n",
       "│    │    │    └─MambaMixer: 4-4         26,112\n",
       "│    │    │    │    └─Conv1d: 5-7        7,680\n",
       "│    │    │    │    └─SiLU: 5-8          --\n",
       "│    │    │    │    └─Linear: 5-9        2,359,296\n",
       "│    │    │    │    └─Linear: 5-10       122,880\n",
       "│    │    │    │    └─Linear: 5-11       75,264\n",
       "│    │    │    │    └─Linear: 5-12       1,179,648\n",
       "│    │    └─MambaBlock: 3-3              --\n",
       "│    │    │    └─MambaRMSNorm: 4-5       768\n",
       "│    │    │    └─MambaMixer: 4-6         26,112\n",
       "│    │    │    │    └─Conv1d: 5-13       7,680\n",
       "│    │    │    │    └─SiLU: 5-14         --\n",
       "│    │    │    │    └─Linear: 5-15       2,359,296\n",
       "│    │    │    │    └─Linear: 5-16       122,880\n",
       "│    │    │    │    └─Linear: 5-17       75,264\n",
       "│    │    │    │    └─Linear: 5-18       1,179,648\n",
       "│    │    └─MambaBlock: 3-4              --\n",
       "│    │    │    └─MambaRMSNorm: 4-7       768\n",
       "│    │    │    └─MambaMixer: 4-8         26,112\n",
       "│    │    │    │    └─Conv1d: 5-19       7,680\n",
       "│    │    │    │    └─SiLU: 5-20         --\n",
       "│    │    │    │    └─Linear: 5-21       2,359,296\n",
       "│    │    │    │    └─Linear: 5-22       122,880\n",
       "│    │    │    │    └─Linear: 5-23       75,264\n",
       "│    │    │    │    └─Linear: 5-24       1,179,648\n",
       "│    │    └─MambaBlock: 3-5              --\n",
       "│    │    │    └─MambaRMSNorm: 4-9       768\n",
       "│    │    │    └─MambaMixer: 4-10        26,112\n",
       "│    │    │    │    └─Conv1d: 5-25       7,680\n",
       "│    │    │    │    └─SiLU: 5-26         --\n",
       "│    │    │    │    └─Linear: 5-27       2,359,296\n",
       "│    │    │    │    └─Linear: 5-28       122,880\n",
       "│    │    │    │    └─Linear: 5-29       75,264\n",
       "│    │    │    │    └─Linear: 5-30       1,179,648\n",
       "│    │    └─MambaBlock: 3-6              --\n",
       "│    │    │    └─MambaRMSNorm: 4-11      768\n",
       "│    │    │    └─MambaMixer: 4-12        26,112\n",
       "│    │    │    │    └─Conv1d: 5-31       7,680\n",
       "│    │    │    │    └─SiLU: 5-32         --\n",
       "│    │    │    │    └─Linear: 5-33       2,359,296\n",
       "│    │    │    │    └─Linear: 5-34       122,880\n",
       "│    │    │    │    └─Linear: 5-35       75,264\n",
       "│    │    │    │    └─Linear: 5-36       1,179,648\n",
       "│    │    └─MambaBlock: 3-7              --\n",
       "│    │    │    └─MambaRMSNorm: 4-13      768\n",
       "│    │    │    └─MambaMixer: 4-14        26,112\n",
       "│    │    │    │    └─Conv1d: 5-37       7,680\n",
       "│    │    │    │    └─SiLU: 5-38         --\n",
       "│    │    │    │    └─Linear: 5-39       2,359,296\n",
       "│    │    │    │    └─Linear: 5-40       122,880\n",
       "│    │    │    │    └─Linear: 5-41       75,264\n",
       "│    │    │    │    └─Linear: 5-42       1,179,648\n",
       "│    │    └─MambaBlock: 3-8              --\n",
       "│    │    │    └─MambaRMSNorm: 4-15      768\n",
       "│    │    │    └─MambaMixer: 4-16        26,112\n",
       "│    │    │    │    └─Conv1d: 5-43       7,680\n",
       "│    │    │    │    └─SiLU: 5-44         --\n",
       "│    │    │    │    └─Linear: 5-45       2,359,296\n",
       "│    │    │    │    └─Linear: 5-46       122,880\n",
       "│    │    │    │    └─Linear: 5-47       75,264\n",
       "│    │    │    │    └─Linear: 5-48       1,179,648\n",
       "│    │    └─MambaBlock: 3-9              --\n",
       "│    │    │    └─MambaRMSNorm: 4-17      768\n",
       "│    │    │    └─MambaMixer: 4-18        26,112\n",
       "│    │    │    │    └─Conv1d: 5-49       7,680\n",
       "│    │    │    │    └─SiLU: 5-50         --\n",
       "│    │    │    │    └─Linear: 5-51       2,359,296\n",
       "│    │    │    │    └─Linear: 5-52       122,880\n",
       "│    │    │    │    └─Linear: 5-53       75,264\n",
       "│    │    │    │    └─Linear: 5-54       1,179,648\n",
       "│    │    └─MambaBlock: 3-10             --\n",
       "│    │    │    └─MambaRMSNorm: 4-19      768\n",
       "│    │    │    └─MambaMixer: 4-20        26,112\n",
       "│    │    │    │    └─Conv1d: 5-55       7,680\n",
       "│    │    │    │    └─SiLU: 5-56         --\n",
       "│    │    │    │    └─Linear: 5-57       2,359,296\n",
       "│    │    │    │    └─Linear: 5-58       122,880\n",
       "│    │    │    │    └─Linear: 5-59       75,264\n",
       "│    │    │    │    └─Linear: 5-60       1,179,648\n",
       "│    │    └─MambaBlock: 3-11             --\n",
       "│    │    │    └─MambaRMSNorm: 4-21      768\n",
       "│    │    │    └─MambaMixer: 4-22        26,112\n",
       "│    │    │    │    └─Conv1d: 5-61       7,680\n",
       "│    │    │    │    └─SiLU: 5-62         --\n",
       "│    │    │    │    └─Linear: 5-63       2,359,296\n",
       "│    │    │    │    └─Linear: 5-64       122,880\n",
       "│    │    │    │    └─Linear: 5-65       75,264\n",
       "│    │    │    │    └─Linear: 5-66       1,179,648\n",
       "│    │    └─MambaBlock: 3-12             --\n",
       "│    │    │    └─MambaRMSNorm: 4-23      768\n",
       "│    │    │    └─MambaMixer: 4-24        26,112\n",
       "│    │    │    │    └─Conv1d: 5-67       7,680\n",
       "│    │    │    │    └─SiLU: 5-68         --\n",
       "│    │    │    │    └─Linear: 5-69       2,359,296\n",
       "│    │    │    │    └─Linear: 5-70       122,880\n",
       "│    │    │    │    └─Linear: 5-71       75,264\n",
       "│    │    │    │    └─Linear: 5-72       1,179,648\n",
       "│    │    └─MambaBlock: 3-13             --\n",
       "│    │    │    └─MambaRMSNorm: 4-25      768\n",
       "│    │    │    └─MambaMixer: 4-26        26,112\n",
       "│    │    │    │    └─Conv1d: 5-73       7,680\n",
       "│    │    │    │    └─SiLU: 5-74         --\n",
       "│    │    │    │    └─Linear: 5-75       2,359,296\n",
       "│    │    │    │    └─Linear: 5-76       122,880\n",
       "│    │    │    │    └─Linear: 5-77       75,264\n",
       "│    │    │    │    └─Linear: 5-78       1,179,648\n",
       "│    │    └─MambaBlock: 3-14             --\n",
       "│    │    │    └─MambaRMSNorm: 4-27      768\n",
       "│    │    │    └─MambaMixer: 4-28        26,112\n",
       "│    │    │    │    └─Conv1d: 5-79       7,680\n",
       "│    │    │    │    └─SiLU: 5-80         --\n",
       "│    │    │    │    └─Linear: 5-81       2,359,296\n",
       "│    │    │    │    └─Linear: 5-82       122,880\n",
       "│    │    │    │    └─Linear: 5-83       75,264\n",
       "│    │    │    │    └─Linear: 5-84       1,179,648\n",
       "│    │    └─MambaBlock: 3-15             --\n",
       "│    │    │    └─MambaRMSNorm: 4-29      768\n",
       "│    │    │    └─MambaMixer: 4-30        26,112\n",
       "│    │    │    │    └─Conv1d: 5-85       7,680\n",
       "│    │    │    │    └─SiLU: 5-86         --\n",
       "│    │    │    │    └─Linear: 5-87       2,359,296\n",
       "│    │    │    │    └─Linear: 5-88       122,880\n",
       "│    │    │    │    └─Linear: 5-89       75,264\n",
       "│    │    │    │    └─Linear: 5-90       1,179,648\n",
       "│    │    └─MambaBlock: 3-16             --\n",
       "│    │    │    └─MambaRMSNorm: 4-31      768\n",
       "│    │    │    └─MambaMixer: 4-32        26,112\n",
       "│    │    │    │    └─Conv1d: 5-91       7,680\n",
       "│    │    │    │    └─SiLU: 5-92         --\n",
       "│    │    │    │    └─Linear: 5-93       2,359,296\n",
       "│    │    │    │    └─Linear: 5-94       122,880\n",
       "│    │    │    │    └─Linear: 5-95       75,264\n",
       "│    │    │    │    └─Linear: 5-96       1,179,648\n",
       "│    │    └─MambaBlock: 3-17             --\n",
       "│    │    │    └─MambaRMSNorm: 4-33      768\n",
       "│    │    │    └─MambaMixer: 4-34        26,112\n",
       "│    │    │    │    └─Conv1d: 5-97       7,680\n",
       "│    │    │    │    └─SiLU: 5-98         --\n",
       "│    │    │    │    └─Linear: 5-99       2,359,296\n",
       "│    │    │    │    └─Linear: 5-100      122,880\n",
       "│    │    │    │    └─Linear: 5-101      75,264\n",
       "│    │    │    │    └─Linear: 5-102      1,179,648\n",
       "│    │    └─MambaBlock: 3-18             --\n",
       "│    │    │    └─MambaRMSNorm: 4-35      768\n",
       "│    │    │    └─MambaMixer: 4-36        26,112\n",
       "│    │    │    │    └─Conv1d: 5-103      7,680\n",
       "│    │    │    │    └─SiLU: 5-104        --\n",
       "│    │    │    │    └─Linear: 5-105      2,359,296\n",
       "│    │    │    │    └─Linear: 5-106      122,880\n",
       "│    │    │    │    └─Linear: 5-107      75,264\n",
       "│    │    │    │    └─Linear: 5-108      1,179,648\n",
       "│    │    └─MambaBlock: 3-19             --\n",
       "│    │    │    └─MambaRMSNorm: 4-37      768\n",
       "│    │    │    └─MambaMixer: 4-38        26,112\n",
       "│    │    │    │    └─Conv1d: 5-109      7,680\n",
       "│    │    │    │    └─SiLU: 5-110        --\n",
       "│    │    │    │    └─Linear: 5-111      2,359,296\n",
       "│    │    │    │    └─Linear: 5-112      122,880\n",
       "│    │    │    │    └─Linear: 5-113      75,264\n",
       "│    │    │    │    └─Linear: 5-114      1,179,648\n",
       "│    │    └─MambaBlock: 3-20             --\n",
       "│    │    │    └─MambaRMSNorm: 4-39      768\n",
       "│    │    │    └─MambaMixer: 4-40        26,112\n",
       "│    │    │    │    └─Conv1d: 5-115      7,680\n",
       "│    │    │    │    └─SiLU: 5-116        --\n",
       "│    │    │    │    └─Linear: 5-117      2,359,296\n",
       "│    │    │    │    └─Linear: 5-118      122,880\n",
       "│    │    │    │    └─Linear: 5-119      75,264\n",
       "│    │    │    │    └─Linear: 5-120      1,179,648\n",
       "│    │    └─MambaBlock: 3-21             --\n",
       "│    │    │    └─MambaRMSNorm: 4-41      768\n",
       "│    │    │    └─MambaMixer: 4-42        26,112\n",
       "│    │    │    │    └─Conv1d: 5-121      7,680\n",
       "│    │    │    │    └─SiLU: 5-122        --\n",
       "│    │    │    │    └─Linear: 5-123      2,359,296\n",
       "│    │    │    │    └─Linear: 5-124      122,880\n",
       "│    │    │    │    └─Linear: 5-125      75,264\n",
       "│    │    │    │    └─Linear: 5-126      1,179,648\n",
       "│    │    └─MambaBlock: 3-22             --\n",
       "│    │    │    └─MambaRMSNorm: 4-43      768\n",
       "│    │    │    └─MambaMixer: 4-44        26,112\n",
       "│    │    │    │    └─Conv1d: 5-127      7,680\n",
       "│    │    │    │    └─SiLU: 5-128        --\n",
       "│    │    │    │    └─Linear: 5-129      2,359,296\n",
       "│    │    │    │    └─Linear: 5-130      122,880\n",
       "│    │    │    │    └─Linear: 5-131      75,264\n",
       "│    │    │    │    └─Linear: 5-132      1,179,648\n",
       "│    │    └─MambaBlock: 3-23             --\n",
       "│    │    │    └─MambaRMSNorm: 4-45      768\n",
       "│    │    │    └─MambaMixer: 4-46        26,112\n",
       "│    │    │    │    └─Conv1d: 5-133      7,680\n",
       "│    │    │    │    └─SiLU: 5-134        --\n",
       "│    │    │    │    └─Linear: 5-135      2,359,296\n",
       "│    │    │    │    └─Linear: 5-136      122,880\n",
       "│    │    │    │    └─Linear: 5-137      75,264\n",
       "│    │    │    │    └─Linear: 5-138      1,179,648\n",
       "│    │    └─MambaBlock: 3-24             --\n",
       "│    │    │    └─MambaRMSNorm: 4-47      768\n",
       "│    │    │    └─MambaMixer: 4-48        26,112\n",
       "│    │    │    │    └─Conv1d: 5-139      7,680\n",
       "│    │    │    │    └─SiLU: 5-140        --\n",
       "│    │    │    │    └─Linear: 5-141      2,359,296\n",
       "│    │    │    │    └─Linear: 5-142      122,880\n",
       "│    │    │    │    └─Linear: 5-143      75,264\n",
       "│    │    │    │    └─Linear: 5-144      1,179,648\n",
       "│    └─MambaRMSNorm: 2-3                 768\n",
       "├─Linear: 1-2                            38,615,040\n",
       "=================================================================\n",
       "Total params: 167,750,400\n",
       "Trainable params: 167,750,400\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mitch/miniconda3/envs/phd/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "class MambaConfig(PretrainedConfig):\n",
    "    model_type = \"mamba\"\n",
    "    def __init__(self, vocab_size=50257, max_position_embeddings=512,\n",
    "                 num_hidden_layers=12, hidden_size=768, num_attention_heads=12,\n",
    "                 intermediate_size=3072, hidden_act=\"gelu\",\n",
    "                 hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1,\n",
    "                 layer_norm_eps=1e-12, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_act = hidden_act\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "config = MambaConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaModel(PreTrainedModel):\n",
    "    config_class = MambaConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([MambaBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        output = self.embeddings(input_ids)\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        output = self.ln_f(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mitch/miniconda3/envs/phd/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n",
      "/home/mitch/miniconda3/envs/phd/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2508/2508 [00:00<00:00, 33819.05 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='1881' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  71/1881 03:24 < 1:29:13, 0.34 it/s, Epoch 0.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.508700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.292300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "model_id = \"state-spaces/mamba-130m-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-3\n",
    ")\n",
    "lora_config =  LoraConfig(\n",
    "        r=8,\n",
    "        target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\"\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"quote\",\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
